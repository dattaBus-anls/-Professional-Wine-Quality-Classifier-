# -*- coding: utf-8 -*-
"""Project_2_Question_1_Wine_quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11qqEiSJMx-cG0LIFOZggUAhlNzecTcSN

üç∑ Wine Quality Classification with Multiple Algorithms  
üìä Dataset: Wine Quality  
üìÅ Course: Image Classification with Ensemble Methods

# 1. Import required libraries
"""

# Importing Liabraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, validation_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import warnings
import time
import pickle
import os
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight

warnings.filterwarnings('ignore')
import shutil

"""# 2. Dataset Loading"""

# Dataset Loading

# Dataset Loading with Real Data Analysis
print("Wine Quality Classification with Multiple Algorithms")
print("="*60)

try:
    # Try to load the real winequalityN.csv dataset
    df = pd.read_csv('winequalityN.csv')
    
    # Standardize column names
    if 'type' in df.columns:
        df['wine_type'] = df['type']
    elif 'wine_type' not in df.columns:
        df['wine_type'] = 'red'  # Default if not specified
    
    print(f"‚úÖ REAL KAGGLE DATASET LOADED!")
    print(f"üìä Total samples: {len(df):,}")


    df.columns = df.columns.str.replace(' ', '_').str.lower()
    print("‚úÖ Column names standardized to use underscores")
    
    print(f"üç∑ Wine types: {df['wine_type'].value_counts().to_dict()}")
    
    # ANALYZE REAL DATA IMBALANCE
    print("\nüìä REAL DATA QUALITY DISTRIBUTION:")
    print("="*60)
    real_quality_dist = df['quality'].value_counts().sort_index()
    for quality, count in real_quality_dist.items():
        percentage = count / len(df) * 100
        print(f"Quality {quality}: {count:,} samples ({percentage:.1f}%)")

except FileNotFoundError:
    print("winequalityN.csv not found. Trying Google Colab upload...")
    
    try:
        # Google Colab file upload
        from google.colab import files
        print("LARGE DATASET OPTION - Combined Wine Quality (6,497 samples)")
        print("Please download and upload from Kaggle:")
        print("Visit: https://www.kaggle.com/datasets/rajyellow46/wine-quality")

        uploaded = files.upload()
        filename = list(uploaded.keys())[0]

        if 'winequalityN' in filename or 'combined' in filename.lower():
            df = pd.read_csv(filename)
            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
            print(f"‚úÖ Combined dataset loaded: {filename}")

        elif 'red' in filename.lower():
            df_red = pd.read_csv(filename, sep=';')
            df_red['type'] = 'red'
            print("Red wine loaded. Please also upload white wine file for larger dataset:")
            uploaded_white = files.upload()
            white_filename = list(uploaded_white.keys())[0]
            df_white = pd.read_csv(white_filename, sep=';')
            df_white['type'] = 'white'
            df = pd.concat([df_red, df_white], ignore_index=True)
            print(f"‚úÖ Combined red and white wine datasets!")

        else:
            df = pd.read_csv(filename, sep=';')
            df['type'] = 'wine'
            print(f"‚úÖ Dataset loaded: {filename}")

        # Standardize column names
        if 'type' not in df.columns and 'wine_type' not in df.columns:
            df['wine_type'] = 'red'
        elif 'type' in df.columns:
            df['wine_type'] = df['type']

        print(f"üìä DATASET LOADED SUCCESSFULLY!")
        print(f"üìà Total samples: {len(df):,}")
        print(f"üç∑ Wine types: {df['wine_type'].value_counts().to_dict()}")

    except Exception as e:
        print(f"Upload failed: {e}")
        print("Creating BALANCED synthetic wine quality dataset...")

        # Create BALANCED synthetic data (not imbalanced like before)
        np.random.seed(42)

        # BALANCED distributions instead of realistic imbalanced ones
        n_red = 1599
        balanced_red_probs = [0.15, 0.20, 0.30, 0.25, 0.08, 0.02]  # BALANCED - For scores 3-8

        red_data = {
            'fixed_acidity': np.clip(np.random.normal(8.32, 1.74, n_red), 4.6, 15.9),
            'volatile_acidity': np.clip(np.random.normal(0.53, 0.18, n_red), 0.12, 1.58),
            'citric_acid': np.clip(np.random.normal(0.27, 0.19, n_red), 0.0, 1.0),
            'residual_sugar': np.clip(np.random.normal(2.54, 1.41, n_red), 0.9, 15.5),
            'chlorides': np.clip(np.random.normal(0.087, 0.047, n_red), 0.012, 0.611),
            'free_sulfur_dioxide': np.clip(np.random.normal(15.87, 10.46, n_red), 1, 72),
            'total_sulfur_dioxide': np.clip(np.random.normal(46.47, 32.9, n_red), 6, 289),
            'density': np.clip(np.random.normal(0.997, 0.002, n_red), 0.99007, 1.00369),
            'pH': np.clip(np.random.normal(3.31, 0.15, n_red), 2.74, 4.01),
            'sulphates': np.clip(np.random.normal(0.66, 0.17, n_red), 0.33, 2.0),
            'alcohol': np.clip(np.random.normal(10.42, 1.07, n_red), 8.4, 14.9),
            'wine_type': 'red'
        }
        red_data['quality'] = np.random.choice(range(3, 9), n_red, p=balanced_red_probs)

        n_white = 4898
        balanced_white_probs = [0.12, 0.18, 0.35, 0.25, 0.08, 0.02]  # BALANCED - For scores 3-8

        white_data = {
            'fixed_acidity': np.clip(np.random.normal(6.85, 0.84, n_white), 3.8, 14.2),
            'volatile_acidity': np.clip(np.random.normal(0.28, 0.10, n_white), 0.08, 1.10),
            'citric_acid': np.clip(np.random.normal(0.33, 0.12, n_white), 0.0, 1.66),
            'residual_sugar': np.clip(np.random.normal(6.39, 5.07, n_white), 0.6, 65.8),
            'chlorides': np.clip(np.random.normal(0.045, 0.022, n_white), 0.009, 0.346),
            'free_sulfur_dioxide': np.clip(np.random.normal(35.31, 17.01, n_white), 2, 289),
            'total_sulfur_dioxide': np.clip(np.random.normal(138.36, 42.50, n_white), 9, 440),
            'density': np.clip(np.random.normal(0.994, 0.003, n_white), 0.98711, 1.03898),
            'pH': np.clip(np.random.normal(3.19, 0.15, n_white), 2.72, 3.82),
            'sulphates': np.clip(np.random.normal(0.49, 0.11, n_white), 0.22, 1.08),
            'alcohol': np.clip(np.random.normal(10.51, 1.23, n_white), 8.0, 14.2),
            'wine_type': 'white'
        }
        white_data['quality'] = np.random.choice(range(3, 9), n_white, p=balanced_white_probs)

        # Combine red and white wine data
        red_df = pd.DataFrame(red_data)
        white_df = pd.DataFrame(white_data)
        df = pd.concat([red_df, white_df], ignore_index=True)

        print("‚úÖ BALANCED synthetic wine dataset created!")
        print(f"üìä Total samples: {len(df):,}")
        print(f"üç∑ Red wines: {len(red_df):,}")
        print(f"üç∑ White wines: {len(white_df):,}")
        print("Note: This synthetic data is BALANCED for better model training.")

"""# 3. Exploring the dataset"""

print(f"Dataset loaded successfully!")
print("="*60)
print(f"Dataset shape: {df.shape}")
print("="*60)
print(f"Columns: {df.columns.tolist()}")
print("="*60)
print("\nDataset Info:")
print("="*60)
print(df.info())
print("="*60)
print("\nFirst 5 rows:")
print("="*60)
print(df.head())
print("="*60)
print("\nDataset Description:")
print("="*60)
print(df.describe())
print("="*60)

"""# 4. Check for missing values

"""

# Check for missing values
print(f"\nCheck for Missing values:")
print("="*60)
print(df.isnull().sum())

"""#5. Group-wise mean imputation to fill the missing values"""

# Fill missing values using wine_type-specific means
df_imputed = df.copy()

for col in ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',
            'chlorides', 'ph', 'sulphates']:

    df_imputed[col] = df.groupby('wine_type')[col].transform(lambda x: x.fillna(x.mean()))

print("‚úÖ Missing values after imputation:")
print("="*60)
print(df_imputed.isnull().sum())

"""# 6. Create quality categories"""

def categorize_quality(score):
    """Convert numerical quality scores to categories"""
    if score <= 4:
        return 'Poor'
    elif score <= 6:
        return 'Average'
    else:
        return 'Good'

# Apply categorization to both dataframes
df['Quality_Category'] = df['quality'].apply(categorize_quality)
df_imputed['Quality_Category'] = df_imputed['quality'].apply(categorize_quality)


# Check target variable distribution
print(f"\nOriginal Quality distribution:")
print("="*60)
print(df['quality'].value_counts().sort_index())
print("="*60)
print(f"Original Quality percentages:")
print("="*60)
print(df['quality'].value_counts(normalize=True).sort_index() * 100)
print("="*60)
print(f"\nCategorized Quality distribution:")
print("="*60)
print(df['Quality_Category'].value_counts())
print("="*60)
print(f"Categorized Quality percentages:")
print("="*60)
print(df['Quality_Category'].value_counts(normalize=True) * 100)

"""# 7. Data Visualizations"""

print("="*60)
print("WINE QUALITY DATA VISUALIZATIONS")
print("="*60)

# ================================================================
# Main Overview
# ================================================================
plt.style.use('default')
fig = plt.figure(figsize=(18, 6))  # Wide figure for 3 boxes in one line

print("Creating Figure 1: Main Overview")
print("="*60)

# Visualization 1: Target distribution
plt.subplot(1, 3, 1)
quality_counts = df['Quality_Category'].value_counts()
plt.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Wine Quality Categories', fontsize=14, pad=20)

# Visualization 2: Correlation heatmap
plt.subplot(1, 3, 2)

# Copy dataframe and encode categorical variables
df_numeric = df.copy()
le_wine_type = LabelEncoder()
df_numeric['wine_type_encoded'] = le_wine_type.fit_transform(df_numeric['wine_type'])
le_quality = LabelEncoder()
df_numeric['Quality_encoded'] = le_quality.fit_transform(df_numeric['Quality_Category'])

# Select numeric columns for correlation
numeric_cols = [
    'fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',
    'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',
    'ph', 'sulphates', 'alcohol', 'wine_type_encoded', 'Quality_encoded'
]

# Compute correlation matrix
correlation_matrix = df_numeric[numeric_cols].corr()

# Draw the heatmap
sns.heatmap(
    correlation_matrix,
    annot=True,
    cmap='coolwarm',
    center=0,
    fmt='.2f',
    square=True,
    linewidths=0.5,
    linecolor='gray',
    xticklabels=correlation_matrix.columns,
    yticklabels=correlation_matrix.columns,
    cbar_kws={"shrink": 0.75}
)

plt.xticks(rotation=45, ha='right', fontsize=8)
plt.yticks(rotation=0, fontsize=8)
plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)

os.makedirs('models', exist_ok=True)
plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight', facecolor='white')

print("‚úÖ Saved: models/correlation_heatmap.png")

# Visualization 3: Alcohol vs Volatile Acidity scatter plot
plt.subplot(1, 3, 3)
colors = ['red', 'blue', 'green', 'orange']
quality_colors = {quality: colors[i] for i, quality in enumerate(df['Quality_Category'].unique())}
for quality in df['Quality_Category'].unique():
    mask = df['Quality_Category'] == quality
    plt.scatter(df[mask]['alcohol'], df[mask]['volatile_acidity'],
               c=quality_colors[quality], alpha=0.6, label=quality, s=30)
plt.xlabel('Alcohol', fontsize=12)
plt.ylabel('Volatile Acidity', fontsize=12)
plt.title('Alcohol vs Volatile Acidity by Quality', fontsize=14, pad=20)
plt.legend()

plt.savefig('models/alcohol_vs_volatile.png', dpi=300, bbox_inches='tight', facecolor='white')
print("‚úÖ Saved: models/alcohol_vs_volatile.png")

plt.tight_layout(pad=3.0)
plt.show()

## checking line for PNG
plt.tight_layout(pad=3.0)
plt.savefig('models/main_overview.png', dpi=300, bbox_inches='tight', facecolor='white')
print("‚úÖ Saved: models/main_overview.png")
plt.show()

print("="*60)
print("Figure 1 completed successfully!")
print("="*60)

# ================================================================
# Feature Distributions
# ================================================================
print("Feature Distributions")
print("="*60)

fig2, axes = plt.subplots(2, 3, figsize=(18, 12))
features = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'ph']

for i, feature in enumerate(features):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    print(f"# Visualization {i+4}: {feature.replace('_', ' ').title()} Distribution")

    for quality in df['Quality_Category'].unique():
        data = df[df['Quality_Category'] == quality][feature]
        ax.hist(data, alpha=0.7, label=quality, bins=20)

    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title(f'Distribution of {feature.replace("_", " ").title()} by Quality', fontsize=13)
    ax.legend()

plt.tight_layout()
plt.show()

print("="*60)
print("Feature Distributions loaded successfully!")
print("="*60)

# ================================================================
#  Additional Analysis
# ================================================================
print("Additional Analysis")
print("="*60)

fig3 = plt.figure(figsize=(18, 6))

# Visualization 10: Alcohol distribution
plt.subplot(1, 3, 1)
for quality in df['Quality_Category'].unique():
    data = df[df['Quality_Category'] == quality]['alcohol']
    plt.hist(data, alpha=0.7, label=quality, bins=20)
plt.xlabel('Alcohol', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Alcohol by Quality', fontsize=14, pad=20)
plt.legend()

# Visualization 11: Box plots
plt.subplot(1, 3, 2)
df_melted = df.melt(id_vars=['Quality_Category'],
                    value_vars=['fixed_acidity', 'volatile_acidity', 'citric_acid', 'alcohol'],
                    var_name='Feature', value_name='Value')
sns.boxplot(data=df_melted, x='Feature', y='Value', hue='Quality_Category')
plt.title('Feature Distributions by Quality', fontsize=14, pad=20)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Values', fontsize=12)

# Visualization 12: Feature means bar plot
plt.subplot(1, 3, 3)
feature_cols = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',
                'chlorides', 'ph', 'sulphates', 'alcohol']
feature_means = df.groupby('Quality_Category')[feature_cols].mean()
feature_means.T.plot(kind='bar', ax=plt.gca())
plt.title('Mean Feature Values by Quality', fontsize=14, pad=20)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Mean Values', fontsize=12)
plt.legend(title='Quality')

plt.tight_layout(pad=3.0)
plt.show()

print("="*60)
print("Figure 3 completed successfully!")
print("="*60)

# ================================================================
# Final Analysis
# ================================================================
print("Final Analysis")
print("="*60)

fig4 = plt.figure(figsize=(18, 6))

# Visualization 13: Citric Acid vs pH scatter plot
plt.subplot(1, 3, 1)
colors = ['red', 'blue', 'green', 'orange']
quality_colors = {quality: colors[i] for i, quality in enumerate(df['Quality_Category'].unique())}
for quality in df['Quality_Category'].unique():
    mask = df['Quality_Category'] == quality
    plt.scatter(df[mask]['citric_acid'], df[mask]['ph'],
               c=quality_colors[quality], alpha=0.6, label=quality)
plt.xlabel('Citric Acid', fontsize=12)
plt.ylabel('pH', fontsize=12)
plt.title('Citric Acid vs pH by Quality', fontsize=14, pad=20)
plt.legend()

# Visualization 14: Sulphates distribution
plt.subplot(1, 3, 2)
for quality in df['Quality_Category'].unique():
    data = df[df['Quality_Category'] == quality]['sulphates']
    plt.hist(data, alpha=0.7, label=quality, bins=20)
plt.xlabel('Sulphates', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Sulphates by Quality', fontsize=14, pad=20)
plt.legend()

# Visualization 15: Density vs Alcohol scatter plot
plt.subplot(1, 3, 3)
for quality in df['Quality_Category'].unique():
    mask = df['Quality_Category'] == quality
    plt.scatter(df[mask]['density'], df[mask]['alcohol'],
               c=quality_colors[quality], alpha=0.6, label=quality)
plt.xlabel('Density', fontsize=12)
plt.ylabel('Alcohol', fontsize=12)
plt.title('Density vs Alcohol by Quality', fontsize=14, pad=20)
plt.legend()

plt.tight_layout(pad=3.0)
plt.show()

print("="*60)
print("Figure 4 completed successfully!")
print("="*60)

# ================================================================
# Summary Function
# ================================================================
def create_custom_distribution_plots(df, features_list, features_per_row=3):
    """Create distribution plots with 3 boxes per line"""

    n_features = len(features_list)
    n_rows = (n_features + features_per_row - 1) // features_per_row

    print("="*60)
    print(f"Creating Custom Distribution Plots ({features_per_row} per line)")
    print("="*60)

    fig, axes = plt.subplots(n_rows, features_per_row, figsize=(18, 6*n_rows))

    # Handle different subplot configurations
    if n_rows == 1 and features_per_row == 1:
        axes = [axes]
    elif n_rows == 1:
        axes = axes.reshape(1, -1)
    elif features_per_row == 1:
        axes = axes.reshape(-1, 1)

    for i, feature in enumerate(features_list):
        row = i // features_per_row
        col = i % features_per_row
        ax = axes[row, col] if n_rows > 1 or features_per_row > 1 else axes[i]

        print(f"# Visualization {i+1}: {feature.replace('_', ' ').title()} Distribution")

        for quality in df['Quality_Category'].unique():
            data = df[df['Quality_Category'] == quality][feature]
            ax.hist(data, alpha=0.7, label=quality, bins=20)

        ax.set_xlabel(feature.replace('_', ' ').title())
        ax.set_ylabel('Frequency')
        ax.set_title(f'Distribution of {feature.replace("_", " ").title()} by Quality')
        ax.legend()

    # Hide empty subplots
    for i in range(n_features, n_rows * features_per_row):
        row = i // features_per_row
        col = i % features_per_row
        if n_rows > 1 or features_per_row > 1:
            axes[row, col].set_visible(False)

    plt.tight_layout()
    plt.show()

    print("="*60)
    print("Custom distribution plots completed!")
    print("="*60)

# Example usage: Create plots for specific features (3 per line)
selected_features = ['alcohol', 'volatile_acidity', 'citric_acid']
create_custom_distribution_plots(df, selected_features, features_per_row=3)

print("="*60)
print("ALL VISUALIZATIONS COMPLETED SUCCESSFULLY!")
print("="*60)
print("üìä Summary:")
print("   - Figure 1: 3 main overview plots (1 line)")
print("   - Figure 2: 6 feature distributions (2 lines, 3 each)")
print("   - Figure 3: 3 additional analysis plots (1 line)")
print("   - Figure 4: 3 final analysis plots (1 line)")
print("   - Custom function: Flexible 3-per-line layouts")
print("="*60)

"""# 8. Data Preprocessing"""

# Prepare features and target
feature_columns = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',
                  'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',
                  'ph', 'sulphates', 'alcohol']

# Add wine type encoding
le_wine = LabelEncoder()
wine_type_encoded = le_wine.fit_transform(df_imputed['wine_type'])

X = df_imputed[feature_columns].copy()
X['wine_type_encoded'] = wine_type_encoded
y = df_imputed['Quality_Category']

# Encode target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"Original classes: {label_encoder.classes_}")
print("="*60)
print(f"Encoded classes: {np.unique(y_encoded)}")
print("="*60)

# Split the data (70:30)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

print("="*60)
print(f"Training set size: {X_train.shape}")
print("="*60)
print(f"Test set size: {X_test.shape}")

"""# 9. Standardization"""

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


print(f"NaN in scaled data: {np.isnan(X_train_scaled).sum()}")
print(f"Shape of scaled data: {X_train_scaled.shape}")
print(f"Original feature means: {X_train.mean().round(3).to_dict()}")
print("="*60)
print(f"Scaled feature means: {np.mean(X_train_scaled, axis=0).round(3)}")
print("="*60)
print(f"Scaled feature stds: {np.std(X_train_scaled, axis=0).round(3)}")

# ================================================================
# STEP 4: CLASS DISTRIBUTION ANALYSIS AND BALANCING
# ================================================================

# CLASS DISTRIBUTION ANALYSIS AND BALANCING
print("\n" + "="*60)
print("CLASS DISTRIBUTION ANALYSIS AND BALANCING")
print("="*60)

# Analyze current class distribution
class_counts = np.bincount(y_train)
class_percentages = class_counts / len(y_train) * 100

print("üìä TRAINING DATA CLASS DISTRIBUTION:")
for i, (class_name, count, percentage) in enumerate(zip(label_encoder.classes_, class_counts, class_percentages)):
    print(f"Class {i} ({class_name}): {count:,} samples ({percentage:.1f}%)")

# Calculate imbalance ratio
majority_class_pct = max(class_percentages)
minority_class_pct = min(class_percentages)
imbalance_ratio = majority_class_pct / minority_class_pct

print(f"\n‚ö†Ô∏è IMBALANCE ANALYSIS:")
print(f"   Majority class: {majority_class_pct:.1f}%")
print(f"   Minority class: {minority_class_pct:.1f}%")
print(f"   Imbalance ratio: {imbalance_ratio:.1f}:1")

# Apply SMOTE if severely imbalanced
if imbalance_ratio > 3:
    print("üö® SEVERE IMBALANCE DETECTED - Applying SMOTE")
    print("-" * 40)
    
    # Apply SMOTE
    smote = SMOTE(random_state=42, k_neighbors=3)
    X_train_scaled_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
    
    print(f"‚úÖ SMOTE Applied:")
    print(f"   Before: {len(X_train_scaled)} samples")
    print(f"   After: {len(X_train_scaled_balanced)} samples")
    
    # Show new distribution
    new_class_counts = np.bincount(y_train_balanced)
    print("\nüìà AFTER SMOTE BALANCING:")
    for i, (class_name, count) in enumerate(zip(label_encoder.classes_, new_class_counts)):
        percentage = count / len(y_train_balanced) * 100
        print(f"Class {i} ({class_name}): {count:,} samples ({percentage:.1f}%)")
    
    # Update training data
    X_train_scaled = X_train_scaled_balanced
    y_train = y_train_balanced
    
    print("üéØ Training will now use BALANCED data!")
    
else:
    print("‚úÖ Acceptable class balance - No SMOTE needed")

print("="*60)

"""# 10. Finding Optimal K for KNN"""

k_range = np.arange(1, 31, 2)  # Testing odd numbers from 1 to 29
print(f"Testing k values: {k_range}")

# Use validation curve to find optimal k
train_scores, val_scores = validation_curve(
    KNeighborsClassifier(), X_train_scaled, y_train,
    param_name='n_neighbors', param_range=k_range,
    cv=5, scoring='accuracy', n_jobs=-1
)

# Calculate mean and std for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# Find optimal k
optimal_k_idx = np.argmax(val_mean)
optimal_k = k_range[optimal_k_idx]

print(f"\nOptimal k value: {optimal_k}")
print(f"Best validation accuracy: {val_mean[optimal_k_idx]:.4f}")

# Plot validation curve for KNN
plt.figure(figsize=(12, 8))
plt.plot(k_range, train_mean, 'o-', color='blue', label='Training Accuracy')
plt.fill_between(k_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(k_range, val_mean, 'o-', color='red', label='Validation Accuracy')
plt.fill_between(k_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
plt.axvline(x=optimal_k, color='green', linestyle='--', label=f'Optimal k = {optimal_k}')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('KNN Validation Curve: Finding Optimal k')

plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('models/knn_validation_curve.png', dpi=300, bbox_inches='tight', facecolor='white')
print("‚úÖ Saved: models/knn_validation_curve.png")
plt.show()

# Additional analysis: Different distance metrics for optimal k
print(f"\nTesting Different Distance Metrics for k={optimal_k}...")

distance_metrics = ['euclidean', 'manhattan', 'minkowski']
distance_scores = {}

for metric in distance_metrics:
    knn = KNeighborsClassifier(n_neighbors=optimal_k, metric=metric)
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')
    distance_scores[metric] = scores.mean()
    print(f"Distance metric '{metric}': CV Accuracy = {scores.mean():.4f}")

best_metric = max(distance_scores, key=distance_scores.get)
print(f"Best distance metric: {best_metric}")

"""# 11. Model Implamentation"""

print("="*60)
print("Model Implamentation")
print("="*60)

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'SVM (Linear)': SVC(kernel='linear', random_state=42),
    'SVM (RBF)': SVC(kernel='rbf', random_state=42),
    'SVM (Polynomial)': SVC(kernel='poly', degree=3, random_state=42),
    'SVM (Sigmoid)': SVC(kernel='sigmoid', random_state=42),
    'SVM (RBF-Tuned)': SVC(kernel='rbf', C=10, gamma='scale', random_state=42),
    'K-Nearest Neighbors (Default)': KNeighborsClassifier(n_neighbors=5),
    f'K-Nearest Neighbors (Optimal k={optimal_k})': KNeighborsClassifier(
        n_neighbors=optimal_k, metric=best_metric
    )
}

# ================================================================
# STEP 5: ADD BALANCED MODELS
# ================================================================

# ADD these balanced models to your existing models dictionary
models.update({
    'Logistic Regression (Balanced)': LogisticRegression(
        random_state=42, max_iter=1000, class_weight='balanced'
    ),
    'Random Forest (Balanced)': RandomForestClassifier(
        random_state=42, n_estimators=100, class_weight='balanced'
    ),
    'SVM RBF (Balanced)': SVC(
        kernel='rbf', random_state=42, class_weight='balanced'
    ),
    'SVM Linear (Balanced)': SVC(
        kernel='linear', random_state=42, class_weight='balanced'
    ),
    'Decision Tree (Balanced)': DecisionTreeClassifier(
        random_state=42, class_weight='balanced'
    )
})

print(f"\nüéØ TOTAL MODELS TO EVALUATE: {len(models)}")
print("Models with balancing:")
for name in models.keys():
    if 'Balanced' in name:
        print(f" - {name}")

print(f"\nModels to be evaluated:")
for name, model in models.items():
    print(f" - {name}")

"""# 12. Cross Validation - 5 Fold CV"""

print("="*60)
print("Cross Validation - 5 Fold CV")
print("="*60)

# Perform cross validation
cv_scores = {}
cv_folds = 5
kfold = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

for name, model in models.items():
    print(f"Evaluating {name}...")
    scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='accuracy')
    cv_scores[name] = scores
    print(f" CV Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
    print("-" * 40)

"""# 13. Cross Validation Result's Graph"""

print("="*60)
print("Cross Validation Result's Graph")
print("="*60)

# Plot cross validation results (REQUIRED GRAPH)
plt.figure(figsize=(16, 8))
model_names = list(cv_scores.keys())
cv_means = [cv_scores[name].mean() for name in model_names]
cv_stds = [cv_scores[name].std() for name in model_names]

plt.bar(range(len(model_names)), cv_means, yerr=cv_stds, capsize=5, alpha=0.7)
plt.xlabel('Models')
plt.ylabel('Cross Validation Accuracy')
plt.title('5-Fold Cross Validation Results')
plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')

plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('models/cv_results.png', dpi=300, bbox_inches='tight', facecolor='white')
print("‚úÖ Saved: models/cv_results.png")
plt.show()

"""# 14. Model Training and Testing"""

print("="*60)
print("Model Training and Testing")
print("="*60)

# dictionaries to store metrics
trained_models = {}
predictions = {}
test_accuracies = {}

for name, model in models.items():
    print(f"Training {name}...")

    # Train the model
    model.fit(X_train_scaled, y_train)
    trained_models[name] = model

    # Make predictions
    y_pred = model.predict(X_test_scaled)
    predictions[name] = y_pred

    # Calculate test accuracy
    test_accuracy = accuracy_score(y_test, y_pred)
    test_accuracies[name] = test_accuracy

    print(f" Test Accuracy: {test_accuracy:.4f}")
    print("-" * 40)

# ================================================================
# STEP 6: PREDICTION DISTRIBUTION ANALYSIS
# ================================================================

# PREDICTION DISTRIBUTION ANALYSIS
print("\n" + "="*60)
print("PREDICTION DISTRIBUTION ANALYSIS")
print("="*60)

print("üéØ This analysis shows if models predict ALL classes or just 'Average':")
print("-" * 60)

for name, y_pred in predictions.items():
    pred_counts = np.bincount(y_pred, minlength=len(label_encoder.classes_))
    total_predictions = len(y_pred)
    
    print(f"\nüìä {name}:")
    prediction_distribution = {}
    
    for i, (class_name, count) in enumerate(zip(label_encoder.classes_, pred_counts)):
        percentage = count / total_predictions * 100 if total_predictions > 0 else 0
        prediction_distribution[class_name] = percentage
        print(f"   {class_name}: {count} predictions ({percentage:.1f}%)")
    
    # Check if model predicts all classes
    min_prediction = min(prediction_distribution.values())
    if min_prediction == 0:
        print(f"   ‚ö†Ô∏è WARNING: This model never predicts some classes!")
    elif min_prediction < 5:
        print(f"   ‚ö†Ô∏è CAUTION: This model rarely predicts minority classes")
    else:
        print(f"   ‚úÖ GOOD: This model predicts all classes!")

print("\n" + "="*60)
print("üéØ LOOK FOR MODELS THAT PREDICT ALL 3 CATEGORIES!")
print("Models with 0% predictions for any category have the imbalance problem.")
print("="*60)

"""# 15. Classification Reports for All Models"""

print("="*60)
print("Classification Reports for all Models")
print("="*60)

# Generate classification reports
classification_reports = {}
performance_metrics = {}

for name in models.keys():
    print("="*60)
    print(f"CLASSIFICATION REPORT - {name}")
    print("="*60)

    # Classification report
    report = classification_report(y_test, predictions[name],
                                 target_names=label_encoder.classes_,
                                 output_dict=True)
    classification_reports[name] = report

    # Print the report
    print(classification_report(y_test, predictions[name],
                              target_names=label_encoder.classes_))

    # Extract performance metrics
    performance_metrics[name] = {
        'accuracy': report['accuracy'],
        'precision': report['weighted avg']['precision'],
        'recall': report['weighted avg']['recall'],
        'f1_score': report['weighted avg']['f1-score']
    }

"""# 16. Performance Metrics Summary"""

print("="*60)
print("Performance Metrics Summary")
print("="*60)

# Create performance metrics summary
metrics_df = pd.DataFrame(performance_metrics).T
print("Performance Metrics Summary:")
print(metrics_df.round(4))

print("="*60)
print("Performance Comparison Visualization")
print("="*60)

# Plot performance metrics (REQUIRED FORMAT FROM ASSIGNMENT)
plt.figure(figsize=(18, 10))

# Create the comparison plot as requested
x = np.arange(len(models))
width = 0.2

plt.bar(x - 1.5*width, metrics_df['accuracy'], width, label='Accuracy', alpha=0.8)
plt.bar(x - 0.5*width, metrics_df['precision'], width, label='Precision', alpha=0.8)
plt.bar(x + 0.5*width, metrics_df['recall'], width, label='Recall', alpha=0.8)
plt.bar(x + 1.5*width, metrics_df['f1_score'], width, label='F1-Score', alpha=0.8)

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Model vs Weighted Avg Of Various Metrics')
plt.xticks(x, metrics_df.index, rotation=45, ha='right')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""# 17. K-MEANS Clustring Analysis"""

print("="*60)
print("K-MEANS Clustring Analysis")
print("="*60)

print("Finding optimal K using Elbow Method...")

# K-means elbow method
k_range = range(1, 11)
inertias = []
fit_times = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    start_time = time.time()
    kmeans.fit(X_train_scaled)
    end_time = time.time()

    inertias.append(kmeans.inertia_)
    fit_times.append(end_time - start_time)
    print(f"k={k}: Inertia={kmeans.inertia_:.2f}, Time={end_time - start_time:.4f}s")

"""# 18. K-MEANS Elbow Plot"""

print("="*60)
print("K-MEANS Elbow Plot")
print("="*60)

# Find the "elbow" point
elbow_k = 3  # Based on 3 quality categories
elbow_score = inertias[elbow_k - 1]

# Create elbow plot (REQUIRED)
fig, ax1 = plt.subplots(figsize=(10, 6))

# Primary axis: distortion score
ax1.plot(k_range, inertias, 'o-', color='blue', label='Inertia (Distortion Score)')
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('Inertia', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_title("K-Means Elbow Method For Optimal Clusters")
ax1.grid(True, alpha=0.3)

# Annotate elbow
ax1.axvline(x=elbow_k, linestyle='--', color='red', alpha=0.7)
ax1.text(elbow_k + 0.1, elbow_score, f'Elbow at k={elbow_k}',
         fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))

# Secondary axis: fit time
ax2 = ax1.twinx()
ax2.plot(k_range, fit_times, 's--', color='green', alpha=0.7, label='Fit Time')
ax2.set_ylabel('Fit Time (seconds)', color='green')
ax2.tick_params(axis='y', labelcolor='green')

plt.tight_layout()
plt.savefig('models/kmeans_elbow.png', dpi=300, bbox_inches='tight', facecolor='white')
print("‚úÖ Saved: models/kmeans_elbow.png")
plt.show()

"""# 19. K-MEANS Model Evaluation"""

print("="*60)
print("K-MEANS Model Evaluation")
print("="*60)

# Apply K-means with optimal K
optimal_k_means = len(np.unique(y_encoded))
print(f"Applying K-Means with k={optimal_k_means}...")

kmeans = KMeans(n_clusters=optimal_k_means, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_train_scaled)

# Add K-means as a "model" for comparison
kmeans_predictions = kmeans.predict(X_test_scaled)
kmeans_accuracy = accuracy_score(y_test, kmeans_predictions)

print(f"K-Means clustering accuracy: {kmeans_accuracy:.4f}")


"""# 19.5. Consensus-Based Model Selection Function (FIXED)"""

def select_best_model_consensus(test_accuracies, predictions, y_test, label_encoder):
    """Select model using consensus ranking approach - FIXED CLASS INDEX ASSUMPTIONS"""
    
    from sklearn.metrics import f1_score, recall_score, accuracy_score
    
    model_evaluation = {}
    
    # CREATE SAFE CLASS-TO-INDEX MAPPING
    class_to_index = {cls: idx for idx, cls in enumerate(label_encoder.classes_)}
    print(f"üîç DEBUG: Class mapping = {class_to_index}")
    
    for model_name in test_accuracies.keys():
        y_pred = predictions[model_name]
        
        # Calculate comprehensive metrics
        accuracy = accuracy_score(y_test, y_pred)
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        
        # FIXED: Safe class-specific recalls using mapping
        class_recalls = recall_score(y_test, y_pred, average=None)
        
        # Safe extraction using class names, not hardcoded indices
        poor_recall = class_recalls[class_to_index['Poor']] if 'Poor' in class_to_index else 0
        good_recall = class_recalls[class_to_index['Good']] if 'Good' in class_to_index else 0
        average_recall = class_recalls[class_to_index['Average']] if 'Average' in class_to_index else 0
        
        min_recall = min(class_recalls)
        
        # Check if predicts all classes
        unique_preds = len(np.unique(y_pred))
        predicts_all = unique_preds == len(label_encoder.classes_)
        
        model_evaluation[model_name] = {
            'accuracy': accuracy,
            'macro_f1': macro_f1,
            'poor_recall': poor_recall,
            'good_recall': good_recall,
            'average_recall': average_recall,
            'min_recall': min_recall,
            'predicts_all_classes': predicts_all
        }
    
    # Consensus ranking
    metrics_to_rank = ['accuracy', 'macro_f1', 'poor_recall', 'good_recall', 'min_recall']
    model_ranks = {}
    
    for metric in metrics_to_rank:
        # Sort models by this metric
        sorted_models = sorted(
            model_evaluation.items(),
            key=lambda x: x[1][metric],
            reverse=True
        )
        
        # Assign ranks
        for rank, (model_name, _) in enumerate(sorted_models):
            if model_name not in model_ranks:
                model_ranks[model_name] = {}
            model_ranks[model_name][metric] = rank
    
    # Calculate total rank (lower is better)
    for model_name in model_ranks:
        # Only consider models that predict all classes
        if model_evaluation[model_name]['predicts_all_classes']:
            model_ranks[model_name]['total_rank'] = sum(model_ranks[model_name].values())
        else:
            model_ranks[model_name]['total_rank'] = 999  # Heavily penalize
    
    # Best model = lowest total rank
    best_model = min(model_ranks, key=lambda x: model_ranks[x]['total_rank'])
    
    print(f"\nüéØ CONSENSUS-BASED MODEL SELECTION:")
    print("=" * 50)
    print(f"üìä Class Order: {list(label_encoder.classes_)}")
    print(f"üìä Class Indices: {class_to_index}")
    print("-" * 50)
    
    for model_name in sorted(model_ranks, key=lambda x: model_ranks[x]['total_rank'])[:5]:
        rank_info = model_ranks[model_name]
        eval_info = model_evaluation[model_name]
        print(f"{model_name}:")
        print(f"  Total Rank: {rank_info['total_rank']}")
        print(f"  Accuracy: {eval_info['accuracy']:.3f}")
        print(f"  Macro F1: {eval_info['macro_f1']:.3f}")
        print(f"  Poor Recall: {eval_info['poor_recall']:.3f}")
        print(f"  Good Recall: {eval_info['good_recall']:.3f}")
        print(f"  Average Recall: {eval_info['average_recall']:.3f}")
        print(f"  Predicts All Classes: {eval_info['predicts_all_classes']}")
        print("-" * 30)
    
    return best_model


"""# 20. Best Model Analysis"""

print("="*60)
print("Best Model Analysis")
print("="*60)

def select_best_reliable_model(test_accuracies):
    """Select best model with preference for balanced versions"""
    
    # First, try to find the best balanced model
    balanced_models = {name: acc for name, acc in test_accuracies.items() if 'Balanced' in name}
    
    if balanced_models:
        best_balanced = max(balanced_models, key=balanced_models.get)
        best_balanced_acc = balanced_models[best_balanced]
        print(f"üéØ Best balanced model: {best_balanced} ({best_balanced_acc:.4f})")
        
        # Check if there's a regular version with same accuracy
        regular_name = best_balanced.replace(' (Balanced)', '')
        if regular_name in test_accuracies:
            regular_acc = test_accuracies[regular_name]
            print(f"üìä Regular version: {regular_name} ({regular_acc:.4f})")
            
            if abs(best_balanced_acc - regular_acc) < 0.001:  # Same accuracy
                print(f"‚úÖ Choosing balanced version for better fairness")
                return best_balanced
        
        return best_balanced
    
    # Fallback to highest accuracy
    return max(test_accuracies, key=test_accuracies.get)

# NEW MODEL SELECTION
best_model_name = select_best_reliable_model(test_accuracies)

# KEEP everything else the same:
best_accuracy = test_accuracies[best_model_name]

print(f"Best performing model: {best_model_name}")
print(f"Best test accuracy: {best_accuracy:.4f}")

print("\nTop 5 performing models:")
print("-" * 40)
sorted_models = sorted(test_accuracies.items(), key=lambda x: x[1], reverse=True)
for i, (name, accuracy) in enumerate(sorted_models[:5], 1):
    print(f"{i}. {name}: {accuracy:.4f}")

"""# 21. Confusion Matrix for Best Model"""

print("="*60)
print("Confusion Matrix for Best Model")
print("="*60)

print(f"Confusion Matrix for Best Model ({best_model_name})...")

# Create confusion matrix
cm = confusion_matrix(y_test, predictions[best_model_name])

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title(f'Confusion Matrix - {best_model_name}')

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig('models/confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')
print("‚úÖ Saved: models/confusion_matrix.png")
plt.show()

"""# 22. Feature Importance Analysis"""

print("="*60)
print("Feature Importance Analysis")
print("="*60)

print("Feature Importance Analysis...")

# Get feature importance for Random Forest

if best_model_name in trained_models and 'Random Forest' in best_model_name:
    rf_model = trained_models[best_model_name]

    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"{best_model_name} Feature Importance:") 
    print(feature_importance)

    plt.figure(figsize=(10, 6))
    plt.bar(feature_importance['feature'], feature_importance['importance'])
    plt.title('Feature Importance (Random Forest)')
    plt.xlabel('Features')
    plt.ylabel('Importance')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('models/feature_importance.png', dpi=300, bbox_inches='tight', facecolor='white')
    print("‚úÖ Saved: models/feature_importance.png")
    plt.show()

"""# 23. Comprehensive Analysis Summary"""

print("="*60)
print("Comprehensive Analysis Summary")
print("="*60)

print(f"Dataset: Wine Quality Classification")
print(f"Total samples: {len(df):,}")
print(f"Features: {len(X.columns)}")
print(f"Classes: {len(label_encoder.classes_)} ({', '.join(label_encoder.classes_)})")

print(f"\nKNN Optimization Results:")
print("-" * 40)
print(f"‚îú‚îÄ‚îÄ Default k=5 accuracy: {test_accuracies['K-Nearest Neighbors (Default)']:.4f}")
print(f"‚îú‚îÄ‚îÄ Optimal k={optimal_k} accuracy: {test_accuracies[f'K-Nearest Neighbors (Optimal k={optimal_k})']:.4f}")
print(f"‚îî‚îÄ‚îÄ Best distance metric: {best_metric}")

print(f"\nBest Overall Model: {best_model_name}")
print(f"Best Accuracy: {best_accuracy:.4f}")

print(f"\nCross Validation Results (Top 5):")
print("-" * 40)
cv_sorted = sorted([(name, scores.mean()) for name, scores in cv_scores.items()],
                  key=lambda x: x[1], reverse=True)
for i, (name, score) in enumerate(cv_sorted[:5], 1):
    print(f"{i}. {name}: {score:.4f}")

print(f"\nTest Accuracy Results (Top 5):")
print("-" * 40)
for i, (name, accuracy) in enumerate(sorted_models[:5], 1):
    print(f"{i}. {name}: {accuracy:.4f}")

print(f"\nK-Means Clustering Accuracy: {kmeans_accuracy:.4f}")

print(f"\nKey Insights:")
print("-" * 40)
if 'Random Forest' in trained_models:
    top_features = feature_importance.head(3)['feature'].tolist()
    print(f"‚îú‚îÄ‚îÄ Most important features: {top_features}")
else:
    print(f"‚îú‚îÄ‚îÄ Most important features: N/A")

print(f"‚îú‚îÄ‚îÄ Best algorithm type: {'SVM' if 'SVM' in best_model_name else best_model_name.split()[0]}")
print(f"‚îî‚îÄ‚îÄ Supervised learning outperformed unsupervised (K-Means) by {best_accuracy - kmeans_accuracy:.4f}")

print("="*60)
print("ANALYSIS COMPLETE! ALL REQUIREMENTS FULFILLED.")
print("="*60)

print("Dataset Information:")
print("-" * 40)
print(f"Public Dataset Link: https://www.kaggle.com/datasets/rajyellow46/wine-quality")
print(f"Dataset Description: Combined red and white wine quality dataset with 6,497 samples")
print(f"Features: 11 chemical properties + wine type")
print(f"Target: Wine quality categories (Poor/Average/Good)")

print("="*60)
print("1. ‚úÖ All 7 algorithms implemented")
print("2. ‚úÖ 5-fold cross validation completed")
print("3. ‚úÖ Classification reports generated")
print("4. ‚úÖ Performance comparison graphs created")
print("5. ‚úÖ K-means elbow plot completed")
print("6. ‚úÖ Best model identified and evaluated")
print("="*60)

"""# 24. Saving Models for Deployment"""

# ================================================================
# Saving Models for Deployment
# ================================================================
print("="*60)
print("Saving Model Componenets For Web Deployment")
print("="*60)
# Create models directory if it doesn't exist
os.makedirs('models', exist_ok=True)

# Save the best performing model
print(f"Saving best model: {best_model_name}")
pickle.dump(trained_models[best_model_name], open('wine_model.pkl', 'wb'))

# Save the scaler (fitted on training data)
print("Saving feature scaler...")
pickle.dump(scaler, open('wine_scaler.pkl', 'wb'))

# Save the label encoder (for target classes)
print("Saving label encoder...")
pickle.dump(label_encoder, open('wine_label_encoder.pkl', 'wb'))

# Save feature names in exact order used during training
print("Saving feature names...")
pickle.dump(X.columns.tolist(), open('wine_feature_names.pkl', 'wb'))

# Save wine type encoder
print("Saving wine type encoder...")
pickle.dump(le_wine, open('wine_type_encoder.pkl', 'wb'))

# Save feature ranges for dynamic sliders
print("Saving feature ranges...")
feature_ranges = {}
for col in X.columns:
    feature_ranges[col] = {
        'min': float(X[col].min()),
        'max': float(X[col].max()),
        'mean': float(X[col].mean()),
        'std': float(X[col].std())
    }
pickle.dump(feature_ranges, open('feature_ranges.pkl', 'wb'))

# Save dataset statistics
print("Saving dataset statistics...")
dataset_stats = {
    'total_samples': len(df),
    'red_wines': len(df[df['wine_type'] == 'red']),
    'white_wines': len(df[df['wine_type'] == 'white']),
    'feature_count': len(X.columns),
    'quality_distribution': df['Quality_Category'].value_counts().to_dict(),
    'best_model_name': best_model_name,
    'best_model_accuracy': best_accuracy,
    'best_model_type': type(trained_models[best_model_name]).__name__
}
pickle.dump(dataset_stats, open('dataset_stats.pkl', 'wb'))

# Save model performance metrics
print("Saving model performance...")
model_performance = {
    'cv_scores': {name: scores.tolist() for name, scores in cv_scores.items()},
    'test_accuracies': test_accuracies,
    'performance_metrics': performance_metrics,
    'classification_reports': classification_reports
}
pickle.dump(model_performance, open('model_performance.pkl', 'wb'))

print("="*60)
print("‚úÖ ALL MODEL COMPONENTS SAVED SUCCESSFULLY!")
print("="*60)
print("Files created:")
print("‚îú‚îÄ‚îÄ wine_model.pkl (Best trained model)")
print("‚îú‚îÄ‚îÄ wine_scaler.pkl (Feature scaler)")
print("‚îú‚îÄ‚îÄ wine_label_encoder.pkl (Target encoder)")
print("‚îú‚îÄ‚îÄ wine_feature_names.pkl (Feature names list)")
print("‚îú‚îÄ‚îÄ wine_type_encoder.pkl (Wine type encoder)")
print("‚îú‚îÄ‚îÄ feature_ranges.pkl (Input validation ranges)")
print("‚îú‚îÄ‚îÄ dataset_stats.pkl (Dataset statistics)")
print("‚îî‚îÄ‚îÄ model_performance.pkl (Model metrics)")
print("="*60)
print("üöÄ Ready for Streamlit deployment!")
print("="*60)

"""# 25. Last file download to local machine"""

# ================================================================
# Downloading all files from Google Colab
# ================================================================
print("="*60)
print("Downloading all files from Google Colab")
print("="*60)

from google.colab import files
import os

# List of all files to download
files_to_download = [
    'wine_model.pkl',
    'wine_scaler.pkl',
    'wine_label_encoder.pkl',
    'wine_feature_names.pkl',
    'wine_type_encoder.pkl',
    'feature_ranges.pkl',
    'dataset_stats.pkl',
    'model_performance.pkl'
]

print("üì• Starting file downloads...")
print("-" * 40)

# Download each file
for file_name in files_to_download:
    if os.path.exists(file_name):
        print(f"‚¨áÔ∏è Downloading {file_name}...")
        files.download(file_name)
        print(f"‚úÖ {file_name} downloaded successfully!")
    else:
        print(f"‚ùå {file_name} not found!")
    print("-" * 20)

print("="*60)
print("üéâ All Files Download Initiated!")
print("="*60)
print("üìã Downloaded Files:")
for file_name in files_to_download:
    if os.path.exists(file_name):
        file_size = os.path.getsize(file_name)
        print(f"‚îú‚îÄ‚îÄ {file_name} ({file_size:,} bytes)")
    else:
        print(f"‚îú‚îÄ‚îÄ {file_name} (‚ùå MISSING)")

print("="*60)

# Also create a requirements.txt file for deployment
requirements_content = """streamlit==1.28.1
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
plotly==5.15.0
pickle-mixin==1.0.2
"""

with open('requirements.txt', 'w') as f:
    f.write(requirements_content)

print("üìÑ Creating requirements.txt for deployment...")
files.download('requirements.txt')
print("‚úÖ requirements.txt downloaded!")

print("="*60)
print("üöÄ Deploymnet Pckages are ready!")
print("="*60)

"""# 26. Downloading all .png files for craeting a readme.md  """

"""# 26. Download PNG Files for README"""

print("="*60)
print("DOWNLOADING PNG FILES FOR README")
print("="*60)

from google.colab import files
import os

# List of PNG files that should exist
png_files = [
    'models/target_distribution_pie.png',
    'models/correlation_heatmap.png',
    'models/alcohol_vs_volatile.png',
    'models/main_overview_plots.png',
    'models/feature_distributions.png',
    'models/knn_validation_curve.png',
    'models/cv_results.png',
    'models/performance_metrics.png',
    'models/kmeans_elbow.png',
    'models/confusion_matrix.png',
    'models/feature_importance.png'
]

print("üìÇ Checking for PNG files...")
existing_files = []

for png_file in png_files:
    if os.path.exists(png_file):
        existing_files.append(png_file)
        file_size = os.path.getsize(png_file)
        print(f"‚úÖ Found: {png_file} ({file_size:,} bytes)")
    else:
        print(f"‚ùå Missing: {png_file}")

# Download existing files
if existing_files:
    print(f"\n‚¨áÔ∏è Starting download of {len(existing_files)} PNG files...")
    for png_file in existing_files:
        print(f"Downloading {png_file}...")
        files.download(png_file)
        print(f"‚úÖ Downloaded!")
    print(f"\nüéâ Successfully downloaded {len(existing_files)} PNG files!")
else:
    print("\n‚ùå No PNG files found to download!")

print("="*60)